---
title: "Social Media Analytics"
author: "Group 9 - Thomas George Thomas, Yang Liu, Pratyush Pothuneedi"
date: "12/8/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE, message=FALSE}
#Importing the required libraries
library(tidytext)
library(tidyverse)
library(stringr)
library(lubridate)
library(ggplot2)
library(dplyr)
library(readr)
library(knitr)
library(factoextra)
library(fpc)
library(clValid)
library(cluster)

```


# 1.Introduction

We take a look at 1.6 million tweets and find interesting patterns s solve our business queries. The techniques used include Text mining, sentimental analysis, probability, building a time series data from the existing data set and clustering related data on various parameters.

## Data Description

The data set contains 1.6 million tweets with the following 6 fields:

- target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)
- ids: The id of the tweet ( 2087)
- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)
- flag: The query (lyx). If there is no query, then this value is NO_QUERY.
- user: the user that tweeted (robotickilldozr)
- text: the text of the tweet (Lyx is cool)

## Data Acquisition

We acquire the data from Kaggle: https://www.kaggle.com/kazanova/sentiment140

```{r, echo=FALSE}
# Social Media data from tweets. We renamed the csv file into tweets from th original file name after extraction for easier readability.
tweetsDataRaw <- read.csv('tweets.csv', header = FALSE)

# Adding Column names
colnames(tweetsDataRaw) <- c("target","ids","date","flag","user","text")
```

Taking 5 rows and a few columns

```{r, echo=FALSE}

kable(
  tweetsDataRaw %>%
  select(ids,date,text) %>%
  slice(0:5)
)
```



# 2.Analytical Questions

## 1. Finding the frequently used unique words

For this insight, we consider only the "original" thought of the user/author. We Remove stop words, username mentions, replies, and Re-tweets so that we only have the "original" tweets and visualize our findings.


```{r, echo=FALSE, message=FALSE}
remove_reg <- "&amp;|&lt;|&gt;"
tidy_tweets <- tweetsDataRaw %>% 
  filter(!str_detect(text, "^(RT|@)")) %>%
  mutate(text = str_remove_all(text, remove_reg)) %>%
  unnest_tokens(word, text, token = "tweets", strip_url = TRUE) %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))
```


```{r fig.align="center", echo=FALSE, message=FALSE}
# counting and sorting the words
tidy_tweets %>%
count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n, fill= word)) +
  geom_col() +
  theme(legend.position="none")+
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Most frequent used unique words in tweets")
```

**Observation:** *Day* is the most frequently used word which has been used around 63,000 times out of the total of 1.6 million tweets. Following that, the words *Time*, *Home*, *love* and *night* have been used around 30,000 times each.

## 2. Sentimental Trends of Tweets

By utilizing the nrc library, we find different sentiments in each of the tweets.

```{r, include=FALSE}
# the lexicon
nrc_lexicon <- get_sentiments("nrc")

# now the job
tidy_tweets <- tidy_tweets %>%
             left_join(nrc_lexicon, by="word")

# remove NA's
tidy_tweets <- tidy_tweets %>%
  filter(sentiment!= "NA")

```


```{r, echo=FALSE, message=FALSE, results='hide',fig.keep='all'}
# Visualizing the results
tidy_tweets %>%
count(sentiment) %>%
  ggplot(aes(x = sentiment, y = n)) +
  geom_bar(aes(fill=sentiment),stat = "identity")+
  theme(legend.position="none")+
  xlab("Sentiments") +
  ylab("Count")+
  ggtitle("Different Sentiments vs Count")
  theme_minimal()

```

**Observation:** Positive, negative, anticipation are the top three most tweeted sentiments. Another trend is that there are equal number of Anger, disgust and surprise sentiment tweets. A lot of Users have have tweeted about issues that they fear and trust.


@Yang Please do this as your 2nd question in Time series
# 3. Extract different months from the date column and determine the sentiments related to the month

Adding the month column to the dataset

```{r}

tidy_tweets <- tidy_tweets %>%
  mutate(elements = str_split(date, fixed(" "), n=6)) %>% 
    mutate(Month = map_chr(elements, 2),
           Day = map_chr(elements, 1),
           date = map_chr(elements, 3),
           Time = map_chr(elements, 4), .keep="unused")

tidy_tweets$date <- as.integer(tidy_tweets$date)

```


# Clustering Analysis
# Demonstrate using visualize function to determine the number of clusters.


```{r}
library(tm)

required_tweets <- data.frame(tidy_tweets$word,tidy_tweets$sentiment)
required_tweets <- required_tweets[50:120, ]

corpus <- Corpus(VectorSource(required_tweets))

tdm <- TermDocumentMatrix(corpus, 
                          control = list(minWordLength=c(1,Inf)))
t <- removeSparseTerms(tdm, sparse=0.98)
m <- as.matrix(t)
m1 <- t(m)



# Hierarchical word/tweet clustering using dendrogram 
distance <- dist(scale(m))
#print(distance, digits = 2)
hc <- hclust(distance, method = "ward.D")
plot(hc, hang=-1)
rect.hclust(hc, k=12)

```


```{r}

library(nonlinearTseries)
page <- read.csv('C:/Users/tgt55/Documents/Northeastern Coursework/Fall 2021/IE5374 - Foundation for Data Analytics Engineering/Projects/Project 2/Social-Media-Analytics-in-R/daily-website-visitors.csv', header = TRUE, sep = ',')


page_df <- data.frame(page$Page.Loads)

colnames(page_df) <- c("Loads")

page_load <- page_df %>% 
  mutate(text = str_remove_all(Loads, ","))

page_load$text <- as.integer(page_load$text)

ts2 <- page_load$text[1000:1200]
ts2 <- ts2/1000
rqa.analysis=rqa(time.series = ts2, embedding.dim=2, time.lag=1,
                 radius=0.1,lmin=1,do.plot=FALSE,distanceToBorder=2)
plot(rqa.analysis)
rqa.analysis




```

